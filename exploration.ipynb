{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d5afc8-8486-45f2-b2b1-5b1d7aa7135e",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5a18fab-8c9c-47b5-815e-5aea6f853959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddef26-399f-4798-a1e1-a10b59896077",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0486a4d3-6094-4699-a2df-13c841910e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tokens(indices, tokenizer, extended, extra_values_pos, strip=True):\n",
    "    if extended:\n",
    "        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else \n",
    "               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\") \n",
    "               for idx in indices]\n",
    "    else:\n",
    "        res = tokenizer.convert_ids_to_tokens(indices)\n",
    "    if strip:\n",
    "        res = list(map(lambda x: x[1:] if x[0] == 'Ġ' else \"#\" + x, res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_tokens(v, k=100, tokenizer=None, only_english=False, only_ascii=False, with_values=False, \n",
    "               exclude_brackets=False, extended=True, extra_values=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    v = deepcopy(v)\n",
    "    ignored_indices = []\n",
    "    if only_ascii:\n",
    "        ignored_indices = [key for val, key in tokenizer.vocab.items() if not val.strip('Ġ').isascii()]\n",
    "    if only_english: \n",
    "        ignored_indices =[key for val, key in tokenizer.vocab.items() if not (val.strip('Ġ').isascii() and val.strip('Ġ[]').isalnum())]\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "    v[ignored_indices] = -np.inf\n",
    "    extra_values_pos = len(v)\n",
    "    if extra_values is not None:\n",
    "        v = torch.cat([v, extra_values])\n",
    "    values, indices = torch.topk(v, k=k)\n",
    "    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos)\n",
    "    if with_values:\n",
    "        res = list(zip(res, values.cpu().numpy()))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_matrix_tokens(mat, k=100, tokenizer=None, rel_thresh=None, thresh=None, \n",
    "                      sample_entries=10000, alphabetical=False, only_english=False,\n",
    "                      exclude_brackets=False, with_values=True, extended=True):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    mat = deepcopy(mat)\n",
    "    ignored_indices = []\n",
    "    if only_english:\n",
    "        ignored_indices = [key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.strip('[]').isalnum())]\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "    mat[ignored_indices, :] = -np.inf\n",
    "    mat[:, ignored_indices] = -np.inf\n",
    "    cond = torch.ones_like(mat).bool()\n",
    "    if rel_thresh:\n",
    "        cond &= (mat > torch.max(mat) * rel_thresh)\n",
    "    if thresh:\n",
    "        cond &= (mat > thresh)\n",
    "    entries = torch.nonzero(cond)\n",
    "    if sample_entries:\n",
    "        entries = entries[np.random.randint(len(torch.nonzero(cond)), size=sample_entries)]\n",
    "    res_indices = sorted(entries, \n",
    "                         key=lambda x: x[0] if alphabetical else -mat[x[0], x[1]])\n",
    "    res = [*map(partial(convert_to_tokens, extended=extended, tokenizer=tokenizer), res_indices)]\n",
    "            \n",
    "    if with_values:\n",
    "        res_ = []\n",
    "        for (x1, x2), (i1, i2) in zip(res, res_indices):\n",
    "            res_.append((x1, x2, mat[i1][i2].item()))\n",
    "        res = res_    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03a094-4602-47bf-89ea-0656fb4c1d5d",
   "metadata": {},
   "source": [
    "## Extract Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e82d2c-92a5-4892-afc3-b40439d6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "emb = gpt.get_output_embeddings().weight.data.T.detach()\n",
    "\n",
    "num_layers = gpt.config.n_layer\n",
    "num_heads = gpt.config.n_head\n",
    "hidden_dim = gpt.config.n_ctx\n",
    "head_size = hidden_dim // num_heads\n",
    "\n",
    "K = torch.cat([gpt.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T\n",
    "                           for j in range(num_layers)]).detach()\n",
    "V = torch.cat([gpt.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n",
    "                           for j in range(num_layers)]).detach()\n",
    "\n",
    "W_Q, W_K, W_V = torch.cat([gpt.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\") \n",
    "                           for j in range(num_layers)]).detach().chunk(3, dim=-1)\n",
    "W_O = torch.cat([gpt.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\") \n",
    "                           for j in range(num_layers)]).detach()\n",
    "\n",
    "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
    "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
    "d_int = K_heads.shape[1]\n",
    "\n",
    "W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)\n",
    "W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4226fa8-213b-47ad-b0f0-ea19f3ead8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to normalize the embeddings\n",
    "K_freq = (F.relu((K_heads @ emb).reshape(-1, len(tokenizer))).mean(0) + 1e-9)\n",
    "V_freq = (F.relu((V_heads @ emb).reshape(-1, len(tokenizer))).mean(0) + 1e-9)\n",
    "W_O_freq = F.relu(abs(W_O_heads @ emb).mean([0, 1, 2]) + 1e-9)\n",
    "W_V_freq = F.relu(abs(W_V_heads.transpose(-1, -2) @ emb).mean([0, 1, 2]) + 1e-9)\n",
    "W_Q_freq = F.relu(abs(W_Q_heads.transpose(-1, -2) @ emb).mean([0, 1, 2]) + 1e-9)\n",
    "W_K_freq = F.relu(abs(W_K_heads.transpose(-1, -2) @ emb).mean([0, 1, 2]) + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b9e38-8ff4-41f6-8c47-8ae23f4293e6",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71b325-da96-47b2-99a8-6b909668b642",
   "metadata": {},
   "source": [
    "### FF Keys & Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b891ff3d-18ac-4daf-bb1b-272cc9bb00a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 170\n",
      "---------------  --------------\n",
      "anywhere         outdoors\n",
      "everywhere       near\n",
      "guidance         north\n",
      "locally          HERE\n",
      "bombs            south\n",
      "wherever         supplies\n",
      "plac             northeast\n",
      "Belfast          here\n",
      "instruction      southeast\n",
      "refuge           east\n",
      "shelter          nearby\n",
      "jihad            southwest\n",
      "elsewhere        Here\n",
      "#jet             somewhere\n",
      "bombed           indoors\n",
      "roaming          northwest\n",
      "in               #near\n",
      "bomb             undecided\n",
      "#sel             herein\n",
      "bombing          #850\n",
      "#opl             ultrasound\n",
      "sitting          west\n",
      "Bomb             anywhere\n",
      "#cl              among\n",
      "#acle            outside\n",
      "forming          statewide\n",
      "banners          elsewhere\n",
      "arriving         northeastern\n",
      "MB               stationary\n",
      "Factory          #lifting\n",
      "pra              #Near\n",
      "#cil             locally\n",
      "travelling       THERE\n",
      "inspiration      upstream\n",
      "assistance       instructors\n",
      "occupying        Interstate\n",
      "Select           concealed\n",
      "camouflage       Somewhere\n",
      "peace            Northeast\n",
      "extras           therein\n",
      "riding           Donetsk\n",
      "at               #Here\n",
      "McGu             smuggled\n",
      "around           northern\n",
      "accompan         Near\n",
      "Peace            #imester\n",
      "Arrows           #insk\n",
      "internationally  #792\n",
      "Assad            nearer\n",
      "somewhere        #south\n",
      "armed            southwestern\n",
      "amongst          #north\n",
      "friendly         southern\n",
      "cartridges       srfAttach\n",
      "#gan             across\n",
      "participation    #502\n",
      "songs            #260\n",
      "wandering        hither\n",
      "discrimination   nationally\n",
      "east             #520\n",
      "Sharia           #Lie\n",
      "among            permits\n",
      "refugees         ammunition\n",
      "Inquiry          #outher\n",
      "away             abroad\n",
      "#Ø               #320\n",
      "#ovo             Joyce\n",
      "beside           #outhern\n",
      "#istas           #ierre\n",
      "#quel            smugglers\n",
      "needle           #outside\n",
      "nearer           #Information\n",
      "graffiti         #here\n",
      "pir              #RNA\n",
      "comparisons      #nance\n",
      "standing         Volunteer\n",
      "peaceful         #selling\n",
      "sponsorship      #POSE\n",
      "camoufl          Instruct\n",
      "#ario            #Supp\n",
      "shop             #vich\n",
      "Sheikh           #among\n",
      "camping          #lift\n",
      "#HL              commercially\n",
      "desert           intermediate\n",
      "on               selfies\n",
      "attending        #osponsors\n",
      "#cal             #RN\n",
      "Medicine         Supp\n",
      "exclusively      hereafter\n",
      "Fighters         #Pierre\n",
      "tours            Infantry\n",
      "Treat            underneath\n",
      "Ard              #YR\n",
      "#mingham         #552\n",
      "outside          Electrical\n",
      "Khal             supply\n",
      "Guid             #inn\n",
      "mel              IEEE\n",
      "Elf              welding\n",
      "amp              potassium\n",
      "settled          #503\n",
      "hanging          Supply\n",
      "instant          Industry\n",
      "downloading      votes\n",
      "#ml              southeastern\n",
      "Peel             #fish\n",
      "footing          #509\n",
      "Help             #506\n",
      "appearing        downstream\n",
      "#ced             adjacent\n",
      "alternative      #751\n",
      "jihadists        #951\n",
      "alongside        secretly\n",
      "beauty           shelter\n",
      "alternatives     northwestern\n",
      "Barg             inside\n",
      "Syrian           donations\n",
      "lying            #SHIP\n",
      "#olan            Springfield\n",
      "centre           beneath\n",
      "#scoring         #NW\n",
      "festival         #INK\n",
      "participating    #ANE\n",
      "browsing         neighboring\n",
      "Palestinian      #anes\n",
      "prec             suppliers\n",
      "miles            chlorine\n",
      "salvation        kilometers\n",
      "#ghan            adjoining\n",
      "#fa              Between\n",
      "amid             moderates\n",
      "idle             Idlib\n",
      "Islamist         located\n",
      "#Bear            Gang\n",
      "injured          #755\n",
      "flyers           #hey\n",
      "Arabic           miles\n",
      "elf              illegally\n",
      "Spells           nearest\n",
      "#ites            sells\n",
      "camps            downtown\n",
      "farming          #191\n",
      "resting          Principles\n",
      "Bean             nationwide\n",
      "Resolution       #assetsadobe\n",
      "#heid            inland\n",
      "abroad           kilometres\n",
      "Shoes            leaflets\n",
      "stickers         Outside\n",
      "healthy          #indust\n",
      "peacefully       #451\n",
      "south            diagrams\n",
      "#stown           Selling\n",
      "leaflets         #OIL\n",
      "bearing          neighbouring\n",
      "upright          downstairs\n",
      "Assistance       scrolls\n",
      "arrive           Insurance\n",
      "#wiki            #785\n",
      "MP               literature\n",
      "LINK             traffickers\n",
      "#abus            engineering\n",
      "driven           Snake\n",
      "Kurdish          #341\n",
      "explosives       Ammunition\n",
      "commuting        Thorn\n",
      "#former          #KN\n",
      "occupation       #WHERE\n",
      "Hawk             facilit\n",
      "#agic            Certification\n",
      "stationed        recruits\n",
      "oil              Lansing\n",
      "#aband           nort\n",
      "icon             #950\n",
      "Bu               #direction\n",
      "ordained         underworld\n",
      "justice          geographically\n",
      "healing          amongst\n",
      "#opolis          Jake\n",
      "exploitation     Jackets\n",
      "entrance         #999\n",
      "nearby           Infrastructure\n",
      "renting          HELP\n",
      "HIV              kilomet\n",
      "rockets          #neath\n",
      "#Ø¯              #bryce\n",
      "#ule             #EngineDebug\n",
      "Shal             #esson\n",
      "signed           #Anna\n",
      "outpost          instruct\n",
      "through          #cffffcc\n",
      "Credits          ammo\n",
      "badges           #gradient\n",
      "training         #supp\n",
      "needles          Consent\n",
      "Discrimination   vendors\n",
      "Intervention     Northwest\n",
      "Ib               federally\n",
      "cleansing        needles\n",
      "---------------  --------------\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = 19, 170\n",
    "print(i1, i2)\n",
    "print(tabulate([*zip(\n",
    "    top_tokens((K_heads[i1, i2]) @ emb.float() / K_freq, k=200),\n",
    "    top_tokens((V_heads[i1, i2]) @ emb.float() / V_freq, k=200),\n",
    ")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec96387-7cd5-4348-9444-6e1a10987da3",
   "metadata": {},
   "source": [
    "### Attention Weights Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee99123-4623-4719-b3a7-0bc274503cf5",
   "metadata": {},
   "source": [
    "**Choose the layer and head here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0d00edd-fdca-40d0-807f-e4bf3d7611e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1, i2 = np.random.randint(num_layers), np.random.randint(num_heads)\n",
    "i1, i2 = 23, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ba526-8409-48e2-bb66-89d29b1418f2",
   "metadata": {},
   "source": [
    "Then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d71e9788-e3f8-4b70-861f-8b9dc0bf891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V_tmp, W_O_tmp = W_V_heads[i1, i2, :], W_O_heads[i1, i2]\n",
    "tmp = ((emb / W_V_freq).T @ (W_V_tmp @ W_O_tmp) @ (emb / W_O_freq))\n",
    "\n",
    "W_Q_tmp, W_K_tmp = W_Q_heads[i1, i2, :], W_K_heads[i1, i2, :]\n",
    "tmp2 = ((emb / W_Q_freq).T @ (W_Q_tmp @ W_K_tmp.T) @ (emb / W_K_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "724f058f-0e14-4d1e-8606-a7dfe575ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_same = False\n",
    "exclude_fuzzy = False\n",
    "reverse_list = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd4e4a-a69b-4d58-a371-426b9073ff81",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $W_{VO}$ Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3fadf-e97d-4798-9aed-e51b7386c876",
   "metadata": {},
   "source": [
    "`th` is the threshold to ease the work of the `torch.topk` operation. This is used to avoid computing the `topk` over the entire matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0d05604-74f8-43d5-a317-99040933214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 200\n",
    "th_max = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b5c1e-ca3c-4f76-ad40-5d18a2422f08",
   "metadata": {},
   "source": [
    "Check how many relevant entries are above `th`. This is important to minimize work done by `topk`. The lower the better (as long as its less that the `k` of the `topk`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3eb578aa-fa98-4907-b153-0267f4e46fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([136, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero((tmp > th) & (tmp < th_max)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac363214-5d6a-4a30-bcae-4b5603722a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_pos = torch.nonzero((tmp > th) & (tmp < th_max)).tolist()\n",
    "if exclude_same:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: tokenizer.decode(x[0]).lower().strip() != tokenizer.decode(x[1]).lower().strip(), \n",
    "        remaining_pos)]\n",
    "if exclude_fuzzy:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: not _fuzzy_eq(tokenizer.decode(x[0]).lower().strip(), tokenizer.decode(x[1]).lower().strip()), \n",
    "        remaining_pos)]\n",
    "    \n",
    "pos_val = tmp2[[*zip(*remaining_pos)]]\n",
    "good_cells = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)]\n",
    "good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
    "remaining_pos_best = np.array(remaining_pos)[torch.argsort(pos_val if reverse_list else -pos_val)[:100]]\n",
    "good_cells_best = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos_best)]\n",
    "# good_cells[:100]\n",
    "# list(zip(good_tokens[0], good_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3aeaea77-3746-4af2-9fd0-aad0336d58a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ortunately', ' inf'),\n",
       " ('hyde', ' De'),\n",
       " ('rored', ' Mir'),\n",
       " ('utsch', 'De'),\n",
       " ('onomous', ' aut'),\n",
       " ('KER', ' Ac'),\n",
       " ('hesion', 'Ad'),\n",
       " ('perors', 'Em'),\n",
       " (' facto', 'De'),\n",
       " ('izabeth', ' Sel'),\n",
       " ('elaide', ' Ad'),\n",
       " ('hesion', ' Ad'),\n",
       " ('obos', ' inf'),\n",
       " ('urnal', ' di'),\n",
       " ('forcer', ' en'),\n",
       " ('ograph', ' aut'),\n",
       " ('urnal', ' Di'),\n",
       " ('isbury', ' Sal'),\n",
       " ('cend', ' Des'),\n",
       " ('amous', ' inf'),\n",
       " ('ortunately', ' unf'),\n",
       " ('rollment', ' en'),\n",
       " (' Portug', ' Em'),\n",
       " ('oustic', ' Ac'),\n",
       " ('umn', ' aut'),\n",
       " ('isbury', 'Sal'),\n",
       " ('legates', ' De'),\n",
       " ('bris', ' Sem'),\n",
       " ('lishes', ' Ab'),\n",
       " ('izontal', ' Hor'),\n",
       " ('azar', ' Sal'),\n",
       " ('hematically', ' Dal'),\n",
       " ('opted', ' Ad'),\n",
       " ('escal', 'De'),\n",
       " (' instincts', ' Ab'),\n",
       " ('phasis', ' Em'),\n",
       " ('opsis', ' Syn'),\n",
       " ('leted', 'De'),\n",
       " ('duction', ' Ab'),\n",
       " ('requent', ' inf'),\n",
       " ('anoia', ' Par'),\n",
       " ('idelity', ' inf'),\n",
       " ('ideo', ' Ir'),\n",
       " ('legate', ' De'),\n",
       " ('aturated', ' Ter'),\n",
       " (' Ad', 'Ad'),\n",
       " ('mercial', ' Sy'),\n",
       " ('ificent', ' Mun'),\n",
       " ('Ad', ' Ad'),\n",
       " (' facto', ' De'),\n",
       " ('plete', ' deg'),\n",
       " ('CRIP', ' des'),\n",
       " ('keley', ' Ber'),\n",
       " ('rylic', ' Ac'),\n",
       " ('imensional', ' Des'),\n",
       " ('perors', ' Em'),\n",
       " (' irreversible', ' Der'),\n",
       " ('utsch', ' De'),\n",
       " ('otions', ' Em'),\n",
       " (' facto', 'de'),\n",
       " ('solete', ' Ab'),\n",
       " ('hibited', ' Pro'),\n",
       " (' capita', ' Per'),\n",
       " (' Ad', ' Ad'),\n",
       " ('ranean', ' Ter'),\n",
       " ('perors', ' em'),\n",
       " ('forcer', ' En'),\n",
       " ('oise', ' Der'),\n",
       " ('olition', ' Ab'),\n",
       " ('reement', ' ag'),\n",
       " ('historic', ' De'),\n",
       " ('cyclop', ' En'),\n",
       " ('ript', ' des'),\n",
       " ('larg', ' En'),\n",
       " ('acus', ' Ab'),\n",
       " ('hyd', 'De'),\n",
       " ('rollment', ' En'),\n",
       " ('emed', ' De'),\n",
       " ('doc', ' ag'),\n",
       " ('IENT', ' Sal'),\n",
       " ('iguous', ' Amb'),\n",
       " ('ourced', ' des'),\n",
       " ('chant', ' en'),\n",
       " ('leted', ' De'),\n",
       " (' pal', ' Em'),\n",
       " ('leted', ' DE'),\n",
       " ('ranean', ' ter'),\n",
       " ('forestation', ' De'),\n",
       " ('escal', ' De'),\n",
       " (' facto', ' de'),\n",
       " ('velop', ' en'),\n",
       " ('raged', ' En'),\n",
       " ('activate', ' De'),\n",
       " (' Di', ' Di'),\n",
       " ('oured', ' Col'),\n",
       " (' Pump', ' Em'),\n",
       " (' residences', ' Em'),\n",
       " ('bing', ' Ab'),\n",
       " ('spe', ' Pro'),\n",
       " ('celer', ' De')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_cells_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49638ac2-455c-4441-aa67-4fde61c9ea83",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $W_{QK}$ Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1d97b-12ed-4945-8614-bf7661893005",
   "metadata": {},
   "source": [
    "Again, `th2` is the threshold to ease the work of the `torch.topk` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a723f9a4-eec8-40b0-8c1d-ab9261d33785",
   "metadata": {},
   "outputs": [],
   "source": [
    "th2 = 20\n",
    "th_max2 = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca3047-a2ed-4b4c-8bc0-ead6c93c94f1",
   "metadata": {},
   "source": [
    "Here again, we count the number of entries above `th2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25b769ef-d9cf-41aa-9de3-348cde429ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29769, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero((tmp2 > th2) & (tmp2 < th_max2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34050ffd-0246-4c8f-bea0-8080681272a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_pos = torch.nonzero((tmp2 > th2) & (tmp2 < th_max2)).tolist()\n",
    "if exclude_same:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: tokenizer.decode(x[0]).lower().strip() != tokenizer.decode(x[1]).lower().strip(), \n",
    "        remaining_pos)]\n",
    "if exclude_fuzzy:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: not _fuzzy_eq(tokenizer.decode(x[0]).lower().strip(), tokenizer.decode(x[1]).lower().strip()), \n",
    "        remaining_pos)]\n",
    "    \n",
    "pos_val = tmp2[[*zip(*remaining_pos)]]\n",
    "good_cells = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)]\n",
    "good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
    "remaining_pos_best = np.array(remaining_pos)[torch.argsort(pos_val if reverse_list else -pos_val)[:100]]\n",
    "good_cells_best = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos_best)]\n",
    "# good_cells[:100]\n",
    "# list(zip(good_tokens[0], good_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "457457d1-691a-461e-8a25-54028fc7c1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', ' bicycle'),\n",
       " ('-', ' platinum'),\n",
       " ('-', ' Velocity'),\n",
       " ('-', 'estine'),\n",
       " ('-', 'acket'),\n",
       " ('-', ' grotesque'),\n",
       " ('-', ' expenditures'),\n",
       " ('-', ' Nah'),\n",
       " ('-', ' festivities'),\n",
       " ('-', ' scrutin'),\n",
       " ('-', ' =>'),\n",
       " ('-', '�'),\n",
       " ('-', ' penalties'),\n",
       " ('-', '789'),\n",
       " ('-', 'ozy'),\n",
       " ('-', ' Size'),\n",
       " ('-', 'Socket'),\n",
       " ('-', ' opening'),\n",
       " ('-', ' withdraw'),\n",
       " ('-', 'Commun'),\n",
       " ('-', 'Although'),\n",
       " ('-', ' lobster'),\n",
       " ('-', 'hp'),\n",
       " ('-', ' rhy'),\n",
       " ('-', 'Utah'),\n",
       " ('-', ' Jun'),\n",
       " ('\\xad', 'transform'),\n",
       " ('-', 'Beh'),\n",
       " ('-', ' Hots'),\n",
       " ('-', ' dr'),\n",
       " ('-', 'Monitor'),\n",
       " ('-', '274'),\n",
       " ('-', ' morning'),\n",
       " ('-', ' Caller'),\n",
       " ('-', ' hunting'),\n",
       " ('-', ' config'),\n",
       " ('-', ' reunited'),\n",
       " ('-', 'neck'),\n",
       " (' and', ' interstate'),\n",
       " ('-', ' jeopardy'),\n",
       " ('-', ' 200'),\n",
       " ('-', '517'),\n",
       " ('-', ' justifies'),\n",
       " ('-', ' singles'),\n",
       " ('-', '672'),\n",
       " ('-', ' gunshot'),\n",
       " ('-', 'innacle'),\n",
       " ('-', 'On'),\n",
       " ('-', ' saves'),\n",
       " ('-', '129'),\n",
       " ('-', ' Cook'),\n",
       " ('-', ' overpowered'),\n",
       " ('-', ' Ten'),\n",
       " ('-', 'Ry'),\n",
       " ('-', 'burning'),\n",
       " ('-', ' plain'),\n",
       " ('-', ' mouse'),\n",
       " (' and', ' bindings'),\n",
       " ('-', 'Streamer'),\n",
       " (' and', ' indu'),\n",
       " (' and', ' nursery'),\n",
       " ('-', 'Amy'),\n",
       " ('-', ' laughing'),\n",
       " (' al', ' potassium'),\n",
       " ('-', ' rookie'),\n",
       " (' and', ' accessories'),\n",
       " (' al', ' tongue'),\n",
       " (' and', ' thinner'),\n",
       " ('-', ' influ'),\n",
       " ('-', 'ensitivity'),\n",
       " ('-', ' Silk'),\n",
       " (' and', 'aque'),\n",
       " (' Al', 'iny'),\n",
       " ('-', ' Brenda'),\n",
       " ('-', ' shifts'),\n",
       " ('-', ' delegated'),\n",
       " ('-', ' Prison'),\n",
       " ('-', 'Writer'),\n",
       " ('-', 'PK'),\n",
       " ('-', 'oup'),\n",
       " ('-', ' consoles'),\n",
       " ('-', ' Learned'),\n",
       " ('-', ' Subaru'),\n",
       " ('-', ' ion'),\n",
       " ('-', ' environmentalists'),\n",
       " ('-', 'Guy'),\n",
       " ('-', ' bicy'),\n",
       " ('-', ' fast'),\n",
       " ('-', ' inmate'),\n",
       " ('-', ' Tape'),\n",
       " (' and', ' Parsons'),\n",
       " ('-', 'Wh'),\n",
       " ('-', '600'),\n",
       " ('-', '>.'),\n",
       " ('-', 'Rich'),\n",
       " ('-', ' overwhelm'),\n",
       " ('-', ' Breaking'),\n",
       " ('-', ' entertain'),\n",
       " ('-', 'arning'),\n",
       " ('-', ' breastfeeding')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_cells_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ae7c2-08cc-4cbc-935c-d8bcb06aca74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
