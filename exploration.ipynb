{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d5afc8-8486-45f2-b2b1-5b1d7aa7135e",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a18fab-8c9c-47b5-815e-5aea6f853959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddef26-399f-4798-a1e1-a10b59896077",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0486a4d3-6094-4699-a2df-13c841910e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tokens(indices, tokenizer, extended, extra_values_pos, strip=True):\n",
    "    if extended:\n",
    "        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else \n",
    "               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\") \n",
    "               for idx in indices]\n",
    "    else:\n",
    "        res = tokenizer.convert_ids_to_tokens(indices)\n",
    "    if strip:\n",
    "        res = list(map(lambda x: x[1:] if x[0] == 'Ġ' else \"#\" + x, res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_tokens(v, k=100, tokenizer=None, only_english=False, only_ascii=True, with_values=False, \n",
    "               exclude_brackets=False, extended=True, extra_values=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    v = deepcopy(v)\n",
    "    ignored_indices = []\n",
    "    if only_ascii:\n",
    "        ignored_indices = [key for val, key in tokenizer.vocab.items() if not val.strip('Ġ').isascii()]\n",
    "    if only_english: \n",
    "        ignored_indices =[key for val, key in tokenizer.vocab.items() if not (val.strip('Ġ').isascii() and val.strip('Ġ[]').isalnum())]\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "    v[ignored_indices] = -np.inf\n",
    "    extra_values_pos = len(v)\n",
    "    if extra_values is not None:\n",
    "        v = torch.cat([v, extra_values])\n",
    "    values, indices = torch.topk(v, k=k)\n",
    "    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos)\n",
    "    if with_values:\n",
    "        res = list(zip(res, values.cpu().numpy()))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_matrix_tokens(mat, k=100, tokenizer=None, rel_thresh=None, thresh=None, \n",
    "                      sample_entries=10000, alphabetical=True, only_english=False,\n",
    "                      exclude_brackets=False, with_values=True, extended=True):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    mat = deepcopy(mat)\n",
    "    ignored_indices = []\n",
    "    if only_english:\n",
    "        ignored_indices = [key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.strip('[]').isalnum())]\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "    mat[ignored_indices, :] = -np.inf\n",
    "    mat[:, ignored_indices] = -np.inf\n",
    "    cond = torch.ones_like(mat).bool()\n",
    "    if rel_thresh:\n",
    "        cond &= (mat > torch.max(mat) * rel_thresh)\n",
    "    if thresh:\n",
    "        cond &= (mat > thresh)\n",
    "    entries = torch.nonzero(cond)\n",
    "    if sample_entries:\n",
    "        entries = entries[np.random.randint(len(torch.nonzero(cond)), size=sample_entries)]\n",
    "    res_indices = sorted(entries, \n",
    "                         key=lambda x: x[0] if alphabetical else -mat[x[0], x[1]])\n",
    "    res = [*map(partial(convert_to_tokens, extended=extended, tokenizer=tokenizer), res_indices)]\n",
    "            \n",
    "    if with_values:\n",
    "        res_ = []\n",
    "        for (x1, x2), (i1, i2) in zip(res, res_indices):\n",
    "            res_.append((x1, x2, mat[i1][i2].item()))\n",
    "        res = res_    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03a094-4602-47bf-89ea-0656fb4c1d5d",
   "metadata": {},
   "source": [
    "## Extract Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e82d2c-92a5-4892-afc3-b40439d6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "emb = gpt.get_output_embeddings().weight.data.T.detach()\n",
    "\n",
    "num_layers = gpt.config.n_layer\n",
    "num_heads = gpt.config.n_head\n",
    "hidden_dim = gpt.config.n_embd\n",
    "head_size = hidden_dim // num_heads\n",
    "\n",
    "K = torch.cat([gpt.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T\n",
    "                           for j in range(num_layers)]).detach()\n",
    "V = torch.cat([gpt.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n",
    "                           for j in range(num_layers)]).detach()\n",
    "\n",
    "W_Q, W_K, W_V = torch.cat([gpt.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\") \n",
    "                           for j in range(num_layers)]).detach().chunk(3, dim=-1)\n",
    "W_O = torch.cat([gpt.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\") \n",
    "                           for j in range(num_layers)]).detach()\n",
    "\n",
    "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
    "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
    "d_int = K_heads.shape[1]\n",
    "\n",
    "W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)\n",
    "W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b9e38-8ff4-41f6-8c47-8ae23f4293e6",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71b325-da96-47b2-99a8-6b909668b642",
   "metadata": {},
   "source": [
    "### FF Keys & Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b891ff3d-18ac-4daf-bb1b-272cc9bb00a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 7\n",
      "----------------  ----------------\n",
      "#problem          issues\n",
      "Problems          issue\n",
      "#Problem          problems\n",
      "problem           problem\n",
      "problems          Issues\n",
      "Problem           #issues\n",
      "woes              Problems\n",
      "Trouble           Problem\n",
      "trouble           #Problem\n",
      "difficulties      trouble\n",
      "#issues           Issue\n",
      "dilemma           #problem\n",
      "deficiencies      #issue\n",
      "#blems            difficulties\n",
      "drawback          troubles\n",
      "shortcomings      #Issue\n",
      "Issues            dilemma\n",
      "defic             woes\n",
      "proble            #Iss\n",
      "#undrum           concerns\n",
      "objection         controversy\n",
      "failings          confusion\n",
      "weakness          flaw\n",
      "weaknesses        dispute\n",
      "disagreements     Trouble\n",
      "issues            disputes\n",
      "objections        dile\n",
      "pitfalls          proble\n",
      "predicament       deficiencies\n",
      "troubles          #blems\n",
      "#errors           deficiency\n",
      "dissatisfaction   concern\n",
      "disagreement      shortcomings\n",
      "worries           conflicts\n",
      "disadvantages     pain\n",
      "#Fail             bug\n",
      "glitches          conflict\n",
      "frustrations      flaws\n",
      "misunderstanding  incomp\n",
      "controversies     difficulty\n",
      "flaws             crisis\n",
      "concerns          stumbling\n",
      "disputes          controversies\n",
      "inconsistencies   Concern\n",
      "sickness          challenges\n",
      "conflic           discrepancy\n",
      "discrepancies     disagreements\n",
      "grievances        crises\n",
      "glitch            headaches\n",
      "#issue            questions\n",
      "drawbacks         discrepancies\n",
      "headaches         bugs\n",
      "#Error            fault\n",
      "disadvantage      pitfalls\n",
      "imbalance         Trou\n",
      "quirks            objections\n",
      "issue             question\n",
      "deficiency        discomfort\n",
      "misfortune        mis\n",
      "ambiguity         Crisis\n",
      "ailments          inconsistencies\n",
      "complaints        ISS\n",
      "problematic       hurdles\n",
      "irregularities    headache\n",
      "fears             differences\n",
      "quarrel           frustrations\n",
      "fiasco            pains\n",
      "#cern             thorn\n",
      "contradictions    disagreement\n",
      "#Failure          worries\n",
      "controversy       hurdle\n",
      "disorder          inadequ\n",
      "confusion         obstacles\n",
      "#risis            disadvantages\n",
      "#ecause           inconsistency\n",
      "dysfunction       glitches\n",
      "discrepancy       grievances\n",
      "limitation        inconvenience\n",
      "dile              inconven\n",
      "nightmare         mismatch\n",
      "flaw              hassle\n",
      "nightmares        snag\n",
      "troublesome       disorder\n",
      "detrim            obstacle\n",
      "mistakes          embarrassment\n",
      "Differences       misunderstanding\n",
      "mistake           errors\n",
      "misunderstand     frustration\n",
      "dispute           shortage\n",
      "#efficiency       disadvantage\n",
      "abnormalities     doubts\n",
      "insecurity        tensions\n",
      "caveats           defects\n",
      "#Exception        Bugs\n",
      "Concern           complaints\n",
      "Paradox           complications\n",
      "vulnerabilities   ambig\n",
      "discrep           error\n",
      "emergencies       anxiety\n",
      "stagnation        challenge\n",
      "limitations       strife\n",
      "instability       weakness\n",
      "reluct            imbalance\n",
      "incompetence      predicament\n",
      "concern           contradictions\n",
      "Failure           ambiguity\n",
      "uncertainties     grievance\n",
      "downside          nightmare\n",
      "debacle           objection\n",
      "#advant           faults\n",
      "#ERROR            insecurity\n",
      "anomalies         sin\n",
      "contradiction     disease\n",
      "frustration       dysfunction\n",
      "criticisms        sickness\n",
      "difficulty        #risis\n",
      "#blem             demons\n",
      "faults            limitations\n",
      "disparities       misconceptions\n",
      "mismatch          shortages\n",
      "inconsistency     limitation\n",
      "misconception     contradiction\n",
      "pecul             conflic\n",
      "blindness         complaint\n",
      "#BUG              disorders\n",
      "differences       complication\n",
      "crises            #chall\n",
      "complication      angst\n",
      "distortions       #Error\n",
      "#Engineers        nightmares\n",
      "shortage          inaccur\n",
      "disparity         injustice\n",
      "#Issue            congestion\n",
      "headache          worry\n",
      "#enhagen          disparity\n",
      "complications     unresolved\n",
      "mish              #BUG\n",
      "shenan            distress\n",
      "imped             pest\n",
      "danger            inequ\n",
      "reluctance        shame\n",
      "Errors            failure\n",
      "#effic            bother\n",
      "complains         mistake\n",
      "discontent        Pain\n",
      "Challenges        shortfall\n",
      "#error            drawbacks\n",
      "#paralle          anomalies\n",
      "shortfall         #bugs\n",
      "tragedy           frag\n",
      "doubts            peril\n",
      "Nightmares        #Questions\n",
      "misconceptions    ailments\n",
      "#rovers           annoyance\n",
      "failure           disaster\n",
      "annoyance         imped\n",
      "complaint         #ophobia\n",
      "#Reason           vulnerabilities\n",
      "challeng          ERROR\n",
      "#foreseen         difference\n",
      "tensions          plight\n",
      "shortages         #Chall\n",
      "#Question         Bug\n",
      "irritation        sins\n",
      "#fail             paradox\n",
      "resentment        mistakes\n",
      "disorders         Questions\n",
      "#ophobia          uncertainties\n",
      "fallacy           uncertainty\n",
      "inacc             inacc\n",
      "#DonaldTrump      mism\n",
      "inequalities      #error\n",
      "violations        failures\n",
      "constraint        tension\n",
      "animosity         barriers\n",
      "Disorder          failings\n",
      "deficits          tragedy\n",
      "scourge           divide\n",
      "question          vulnerability\n",
      "ambig             misunderstand\n",
      "inexper           Disease\n",
      "#obia             caveats\n",
      "catastrophe       QUEST\n",
      "Puzz              qual\n",
      "defect            Errors\n",
      "#emonium          Fault\n",
      "#umsy             irregularities\n",
      "cumbers           #effic\n",
      "errors            Anxiety\n",
      "defects           gaps\n",
      "ERROR             disparities\n",
      "#itis             matters\n",
      "hesitation        bottleneck\n",
      "aversion          abuses\n",
      "discomfort        distortion\n",
      "Shame             #lict\n",
      "malfunction       nuisance\n",
      "predic            #pain\n",
      "obstacles         hardships\n",
      "#QUEST            drawback\n",
      "----------------  ----------------\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = 21, 7\n",
    "print(i1, i2)\n",
    "print(tabulate([*zip(\n",
    "    top_tokens((K_heads[i1, i2]) @ emb, k=200),\n",
    "    top_tokens((V_heads[i1, i2]) @ emb, k=200),\n",
    ")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec96387-7cd5-4348-9444-6e1a10987da3",
   "metadata": {},
   "source": [
    "### Attention Weights Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd4e4a-a69b-4d58-a371-426b9073ff81",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $W_{VO}$ Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79156807-54af-4fe8-b170-29dc0856d686",
   "metadata": {},
   "source": [
    "Choose **layer** and **head** here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d00edd-fdca-40d0-807f-e4bf3d7611e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i1, i2 = np.random.randint(num_layers), np.random.randint(num_heads)\n",
    "i1, i2 = 18, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc5d25-0f52-4072-8e66-22808a7464da",
   "metadata": {},
   "source": [
    "Then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2743e277-74f7-41a6-a479-5bd65840d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V_tmp, W_O_tmp = W_V_heads[i1, i2, :], W_O_heads[i1, i2]\n",
    "tmp = (emb.T @ (W_V_tmp @ W_O_tmp) @ emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba073c-01ff-4f3d-8e30-d65cd26803d0",
   "metadata": {},
   "source": [
    "**Compute interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3fadf-e97d-4798-9aed-e51b7386c876",
   "metadata": {},
   "source": [
    "`th` is the threshold to ease the work of the `torch.topk` operation. This is used to avoid computing the `topk` over the entire matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d05604-74f8-43d5-a317-99040933214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 25\n",
    "th_max = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b5c1e-ca3c-4f76-ad40-5d18a2422f08",
   "metadata": {},
   "source": [
    "Check how many relevant entries are above `th`. This is important to minimize work done by `topk`. The lower the better (as long as its less that the `k` of the `topk`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb578aa-fa98-4907-b153-0267f4e46fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero((tmp > th) & (tmp < th_max)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3fe6aa-d353-40c7-a2ff-ba9065b08b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_same = False\n",
    "exclude_fuzzy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072a80da-6a79-4ab1-83da-89c882923c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_list = False\n",
    "only_ascii = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac363214-5d6a-4a30-bcae-4b5603722a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_pos = torch.nonzero((tmp > th) & (tmp < th_max)).tolist()\n",
    "if only_ascii:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: (tokenizer.decode(x[0]).strip('Ġ').isascii() and tokenizer.decode(x[1]).strip('Ġ').isascii()), \n",
    "        remaining_pos)]\n",
    "if exclude_same:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: tokenizer.decode(x[0]).lower().strip() != tokenizer.decode(x[1]).lower().strip(), \n",
    "        remaining_pos)]\n",
    "if exclude_fuzzy:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: not _fuzzy_eq(tokenizer.decode(x[0]).lower().strip(), tokenizer.decode(x[1]).lower().strip()), \n",
    "        remaining_pos)]\n",
    "    \n",
    "pos_val = tmp[[*zip(*remaining_pos)]]\n",
    "good_cells = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)]\n",
    "good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
    "remaining_pos_best = np.array(remaining_pos)[torch.argsort(pos_val if reverse_list else -pos_val)[:50]]\n",
    "good_cells_best = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos_best)]\n",
    "# good_cells[:100]\n",
    "# list(zip(good_tokens[0], good_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aeaea77-3746-4af2-9fd0-aad0336d58a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' herself', ' her'),\n",
       " (' herself', ' herself'),\n",
       " (' herself', ' she'),\n",
       " (' herself', ' hers'),\n",
       " (' herself', 'She'),\n",
       " (' herself', 'she'),\n",
       " (' herself', 'Her'),\n",
       " (' himself', 'his'),\n",
       " (' herself', ' She'),\n",
       " (' itself', 'Its'),\n",
       " (' herself', ' SHE'),\n",
       " (' himself', 'His'),\n",
       " (' himself', ' his'),\n",
       " (' themselves', 'their'),\n",
       " (' herself', ' HER'),\n",
       " (' herself', 'her'),\n",
       " (' herself', ' Her'),\n",
       " (' themselves', 'Their'),\n",
       " (' itself', ' Its'),\n",
       " (' himself', ' he'),\n",
       " (' themselves', ' THEIR'),\n",
       " (' himself', ' himself'),\n",
       " (' himself', 'He'),\n",
       " (' themselves', ' Their'),\n",
       " (' himself', 'him'),\n",
       " (' itself', ' its'),\n",
       " (' themselves', 'They'),\n",
       " (' himself', ' HIS'),\n",
       " (' Himself', 'His'),\n",
       " (' Himself', 'his'),\n",
       " (' themselves', 'they'),\n",
       " (' themselves', ' their'),\n",
       " (' yourselves', ' yourselves'),\n",
       " ('uel', 'Its'),\n",
       " ('his', 'his'),\n",
       " (' himself', ' him'),\n",
       " (' themselves', ' themselves'),\n",
       " (' themselves', ' theirs'),\n",
       " (' himself', ' His'),\n",
       " (' themselves', ' They'),\n",
       " ('MH', 'Its'),\n",
       " (' ITS', 'Its'),\n",
       " ('his', 'His'),\n",
       " ('yssey', 'Its'),\n",
       " (' husband', ' her')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_cells_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49638ac2-455c-4441-aa67-4fde61c9ea83",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $W_{QK}$ Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfc882-3bf7-46e5-8e6a-f6a245ceb7bf",
   "metadata": {},
   "source": [
    "Choose **layer** and **head** here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43e9c99-f9bf-4eae-b215-289a7f630ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i1, i2 = np.random.randint(num_layers), np.random.randint(num_heads)\n",
    "i1, i2 = 20, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094ab90-3daa-45cb-b8fd-e3346222b5ac",
   "metadata": {},
   "source": [
    "Then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e74cb0c2-c39f-42ce-a87c-91125207d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q_tmp, W_K_tmp = W_Q_heads[i1, i2, :], W_K_heads[i1, i2, :]\n",
    "tmp2 = ((emb).T @ (W_Q_tmp @ W_K_tmp.T) @ (emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb2f13-97ff-4d75-89d1-27f49c26df74",
   "metadata": {},
   "source": [
    "**Compute interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1d97b-12ed-4945-8614-bf7661893005",
   "metadata": {},
   "source": [
    "Again, `th2` is the threshold to ease the work of the `torch.topk` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a723f9a4-eec8-40b0-8c1d-ab9261d33785",
   "metadata": {},
   "outputs": [],
   "source": [
    "th2 = 2\n",
    "th_max2 = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25b769ef-d9cf-41aa-9de3-348cde429ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46087, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero((tmp2 > th2) & (tmp2 < th_max2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08d7125c-a5fe-4a8e-9c83-ba370daf1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_same = False\n",
    "exclude_fuzzy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52650f6c-3d14-4a04-9e4c-538dbcbe5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_list = False\n",
    "only_ascii = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34050ffd-0246-4c8f-bea0-8080681272a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_pos = torch.nonzero((tmp2 > th2) & (tmp2 < th_max2)).tolist()\n",
    "if only_ascii:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: (tokenizer.decode(x[0]).strip('Ġ').isascii() and tokenizer.decode(x[1]).strip('Ġ').isascii()), \n",
    "        remaining_pos)]\n",
    "if exclude_same:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: tokenizer.decode(x[0]).lower().strip() != tokenizer.decode(x[1]).lower().strip(), \n",
    "        remaining_pos)]\n",
    "if exclude_fuzzy:\n",
    "    remaining_pos = [*filter(\n",
    "        lambda x: not _fuzzy_eq(tokenizer.decode(x[0]).lower().strip(), tokenizer.decode(x[1]).lower().strip()), \n",
    "        remaining_pos)]\n",
    "    \n",
    "pos_val = tmp2[[*zip(*remaining_pos)]]\n",
    "good_cells = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)]\n",
    "good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
    "remaining_pos_best = np.array(remaining_pos)[torch.argsort(pos_val if reverse_list else -pos_val)[:50]]\n",
    "good_cells_best = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos_best)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "457457d1-691a-461e-8a25-54028fc7c1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('...\"', '...\"'),\n",
       " (' [', ' ['),\n",
       " ('...', '...\"'),\n",
       " ('...\"', '...\"'),\n",
       " ('...\"', ' ['),\n",
       " (' [', '...\"'),\n",
       " ('...', ' ['),\n",
       " ('...\"', ' ['),\n",
       " (' [...]', ' ['),\n",
       " ('...', ' [...]'),\n",
       " ('...\"', ' [...]'),\n",
       " (' [', '[/'),\n",
       " ('...', ')\",'),\n",
       " (' [', '%\"'),\n",
       " (' [', ' [...]'),\n",
       " (' [', ').\"'),\n",
       " (' [', \"['\"),\n",
       " ('...', '%\"'),\n",
       " (' [', \" ['\"),\n",
       " (' [...]', '...\"'),\n",
       " ('...', '...\"'),\n",
       " ('.\"', '...\"'),\n",
       " ('...', '...\"'),\n",
       " ('.\"', ' ['),\n",
       " ('.\"', '...\"'),\n",
       " (' [', '...\"'),\n",
       " ('...', '[/'),\n",
       " ('...\"', '.\"'),\n",
       " (' [', '\\'\"'),\n",
       " (' [', '!\"'),\n",
       " ('...\"', ' [...]'),\n",
       " ('!\"', ' ['),\n",
       " (' [', ')\",'),\n",
       " (' [...]', ' [...]'),\n",
       " ('...', ')\",'),\n",
       " (' [', '.\"['),\n",
       " (' [', '.\",'),\n",
       " (' [', '\";'),\n",
       " ('ind', 'iph'),\n",
       " ('...', '\\'\"'),\n",
       " (' [', '\":'),\n",
       " (',\"', ' ['),\n",
       " (' [', ',\"'),\n",
       " (' [', '),\"'),\n",
       " (' [', '\\',\"'),\n",
       " ('...\"', ').\"'),\n",
       " ('...\"', '...\"'),\n",
       " ('...\"', '...\"'),\n",
       " ('...', '\";'),\n",
       " ('...', ' </')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_cells_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
