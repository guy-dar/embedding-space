{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4c816e-f4ca-43ba-8000-c93942620147",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd22ca7-dbdd-4855-83ac-405efbf1e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from transformers import (AutoModelForMaskedLM, AutoModelForCausalLM, AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          AutoModelForSequenceClassification, TrainingArguments, Trainer)\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "from datasets import load_dataset, load_metric\n",
    "import os\n",
    "from utils import top_tokens\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2ad759-4d6f-4381-9148-e038e0d48f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_output_emb(model):\n",
    "#     transform = model.cls.predictions.transform.dense.weight.T\n",
    "#     orig_emb = model.get_output_embeddings().weight.T\n",
    "#     return (transform @ orig_emb).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07fdd79f-7d70-4548-8b9c-909cc86d81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2') # ('bert-base-uncased') # get_multiberts_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14b9d71-1578-402e-bb0e-e343f0cbd78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2AvgClassifier(nn.Module):\n",
    "    def __init__(self, name, freeze=None, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(name, num_labels=num_labels)\n",
    "        self.model.transformer.ln_f = nn.Identity(self.model.config.n_ctx)\n",
    "        if freeze is not None:\n",
    "            for n, p in self.named_parameters():\n",
    "                p.requires_grad = False\n",
    "                if len(n.split('.transformer.h.')) == 2 and n.endswith('.weight'):\n",
    "                    if int(n.split('.transformer.h.')[1].split('.')[0]) >= freeze:\n",
    "                        p.requires_grad = True\n",
    "                        print(n)\n",
    "                if n.endswith('.classifier.weight'):\n",
    "                    p.requires_grad = True\n",
    "                    print(n)\n",
    "                    \n",
    "    def forward(self, input_ids, labels, inputs_embeds=None):\n",
    "        res = self.model(input_ids=input_ids, inputs_embeds=inputs_embeds)\n",
    "        res.logits = res.logits.mean(dim=-2)\n",
    "        res['loss'] = F.cross_entropy(res.logits.view(-1, res.logits.shape[-1]), labels.view(-1))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d2819-e575-4f98-83af-1cd016b88bce",
   "metadata": {},
   "source": [
    "### Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "601e4a50-cf00-4245-a62e-421113db425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze = 9 # number of layers to freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ace372-e479-4caa-bcf3-68043ebeb0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt2', 'gpt2-medium']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.attn.masked_bias', 'h.9.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'classifier.bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'classifier.weight', 'h.11.attn.masked_bias', 'h.0.attn.masked_bias', 'h.2.attn.masked_bias', 'h.8.attn.masked_bias', 'h.10.attn.masked_bias', 'h.1.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.transformer.h.9.ln_1.weight\n",
      "model.transformer.h.9.attn.c_attn.weight\n",
      "model.transformer.h.9.attn.c_proj.weight\n",
      "model.transformer.h.9.ln_2.weight\n",
      "model.transformer.h.9.mlp.c_fc.weight\n",
      "model.transformer.h.9.mlp.c_proj.weight\n",
      "model.transformer.h.10.ln_1.weight\n",
      "model.transformer.h.10.attn.c_attn.weight\n",
      "model.transformer.h.10.attn.c_proj.weight\n",
      "model.transformer.h.10.ln_2.weight\n",
      "model.transformer.h.10.mlp.c_fc.weight\n",
      "model.transformer.h.10.mlp.c_proj.weight\n",
      "model.transformer.h.11.ln_1.weight\n",
      "model.transformer.h.11.attn.c_attn.weight\n",
      "model.transformer.h.11.attn.c_proj.weight\n",
      "model.transformer.h.11.ln_2.weight\n",
      "model.transformer.h.11.mlp.c_fc.weight\n",
      "model.transformer.h.11.mlp.c_proj.weight\n",
      "model.classifier.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_paths = ['gpt2', 'gpt2-medium'] \n",
    "\n",
    "print(model_paths)\n",
    "\n",
    "model1 = Gpt2AvgClassifier(model_paths[0], freeze=freeze) # AutoModelForSequenceClassification.from_pretrained(model_paths[0])\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(model_paths[1])\n",
    "# we can use input embedding as the embedding matrices are tied\n",
    "emb1 = model1.model.get_input_embeddings().weight.T.cpu().detach() \n",
    "emb2 = model2.get_input_embeddings().weight.T.cpu().detach() \n",
    "num_layers1, hidden_dim1 = (model1.model.config.n_layer, model1.model.config.n_embd)\n",
    "num_layers2, hidden_dim2 = (model2.config.n_layer, model2.config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba62929-5478-467a-976d-2b04ca1ccb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_paths = ['gpt2', 'gpt2-medium'] # [\"bert-base-uncased\", f\"multiberts/models/seed_0\"] # [f\"multiberts/models/seed_{i}\" for i in range(2)]\n",
    "# print(model_paths)\n",
    "\n",
    "# model1_tmp = AutoModelForCausalLM.from_pretrained(model_paths[0]) # need to get the output emb matrix from model1\n",
    "# model2 = AutoModelForCausalLM.from_pretrained(model_paths[1])\n",
    "# emb1, emb2 = map(lambda model: model.get_output_embeddings().weight.T.cpu().detach(), [model1_tmp, model2])\n",
    "# del model1_tmp # now we no longer need model1_tmp\n",
    "# model1 = AutoModelForSequenceClassification.from_pretrained(model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82200f0b-6d47-43f5-a9ea-3ef63f7a3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove pooler for simplicity - leave only classifier \n",
    "# model1.bert.pooler.dense = nn.Identity()\n",
    "# model1.bert.pooler.activation = nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0f339-4a8e-4780-be94-40a3ffd9cef2",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bcbc0b-3ee5-4494-9e92-1a65504e7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1997c9a5-6331-441f-b02b-1f05e18a80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(\"unfrozen parameters:\")\n",
    "    learn_bias = True\n",
    "    for n, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "        if len(n.split('transformer.h.')) == 2 and (learn_bias or n.endswith('.weight')): # '.encoder.layer.'\n",
    "            if int(n.split('transformer.h.')[1].split('.')[0]) >= freeze:\n",
    "                p.requires_grad = True\n",
    "                print(n)\n",
    "        if 'score' in n and (learn_bias or n.endswith('.classifier.weight')): # 'classifier'\n",
    "            p.requires_grad = True\n",
    "            print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41f0eb-e58f-45f8-9cf3-f4aff4e6c100",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a17d09-8034-4df8-b1a0-62ff7814598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_imdb(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46bcd56a-e0f0-4bd4-abb7-7f43dde6e29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/guydar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0632b90a24ff4d20805789aee2e32a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/guydar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-3524c89eaed1ab3e.arrow\n",
      "Loading cached processed dataset at /home/guydar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-bd746f6e0438ac54.arrow\n",
      "Loading cached processed dataset at /home/guydar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-fcdd099119f0e220.arrow\n",
      "Loading cached shuffled indices for dataset at /home/guydar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-8a65a18bdcacec47.arrow\n",
      "Loading cached shuffled indices for dataset at /home/guydar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-1e8bb50c418695d3.arrow\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset('imdb')\n",
    "imdb = imdb.map(tokenize_imdb, batched=False)\n",
    "imdb_train, imdb_val = imdb['train'].shuffle(seed=42).select(range(1000)), imdb['test'].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca5ca0-367c-4a78-90cf-fe550e4dd346",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3460654a-3f6b-4469-87ab-123ab78abccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3afd2645-9df9-4185-9219-619b6623b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66a63fc7-d14c-484b-b4cf-e7223fd23997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "train_args = TrainingArguments(learning_rate=1e-5, report_to=None, output_dir='trainer_output', \n",
    "                               per_device_eval_batch_size=1, per_device_train_batch_size=1, \n",
    "                               save_steps=False, evaluation_strategy='epoch', num_train_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38f57c62-7d19-40b0-bcd0-2a869ae0d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args._n_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9a30474-c98a-46ff-9925-851652170702",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432e8261-d5a1-468e-a62e-c1a79a5324be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Gpt2AvgClassifier.forward` and have been ignored: attention_mask, text. If attention_mask, text are not expected by `Gpt2AvgClassifier.forward`,  you can safely ignore this message.\n",
      "/mnt/netapp7/dar/miniconda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 01:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.623600</td>\n",
       "      <td>1.006617</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.646500</td>\n",
       "      <td>0.787116</td>\n",
       "      <td>0.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.593100</td>\n",
       "      <td>0.736443</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Gpt2AvgClassifier.forward` and have been ignored: attention_mask, text. If attention_mask, text are not expected by `Gpt2AvgClassifier.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Gpt2AvgClassifier.forward` and have been ignored: attention_mask, text. If attention_mask, text are not expected by `Gpt2AvgClassifier.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Gpt2AvgClassifier.forward` and have been ignored: attention_mask, text. If attention_mask, text are not expected by `Gpt2AvgClassifier.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.875273193359375, metrics={'train_runtime': 109.7321, 'train_samples_per_second': 27.339, 'train_steps_per_second': 27.339, 'total_flos': 0.0, 'train_loss': 0.875273193359375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model, args=train_args, train_dataset=imdb_train, eval_dataset=imdb_val, \n",
    "                  compute_metrics=compute_metrics)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685ad43-e590-4e53-b776-cce678426ca8",
   "metadata": {},
   "source": [
    "### Visualize Finetuning Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04135c28-54aa-4097-8ade-34f99d667f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_classifier = model.model.classifier.weight.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7cc0792-d95a-4418-a58d-5999a2045af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_classifier = (model.model.classifier.weight.cpu() - old_model.model.classifier.weight).detach()\n",
    "# diff_classifier = model.score.weight.detach().cpu() - old_model.score.weight.detach()\n",
    "# diff_classifier = model.classifier.weight.detach().cpu() - old_model.classifier.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e949f1f-b149-4ee3-b521-d7f262aa563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_vector = diff_classifier[0, :]\n",
    "pos_vector = diff_classifier[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "557b80ff-3285-4ba3-9397-0ac00d46ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE     NEGATIVE\n",
      "-----------  ------------\n",
      "#yssey       bullshit\n",
      "#knit        lame\n",
      "#etts        crap\n",
      "passions     incompetent\n",
      "#etooth      inco\n",
      "#iscover     bland\n",
      "pioneers     incompetence\n",
      "#emaker      idiots\n",
      "Pione        crappy\n",
      "#raft        shitty\n",
      "#uala        idiot\n",
      "prosper      pointless\n",
      "#izons       retarded\n",
      "#encers      worse\n",
      "#joy         garbage\n",
      "cherish      CGI\n",
      "loves        FUCK\n",
      "#accompan    Nope\n",
      "strengthens  useless\n",
      "#nect        shit\n",
      "comr         mediocre\n",
      "honoured     poorly\n",
      "insepar      stupid\n",
      "embraces     inept\n",
      "battled      lousy\n",
      "#Together    fuck\n",
      "intrig       sloppy\n",
      "#jong        Worse\n",
      "friendships  Worst\n",
      "#anta        meaningless\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(\n",
    "    [*zip(*[top_tokens(pos_vector @ emb1, k=30, only_ascii=True, tokenizer=tokenizer),\n",
    "            top_tokens(neg_vector @ emb1, k=30, only_ascii=True, tokenizer=tokenizer)])],\n",
    "     headers=['POSITIVE', 'NEGATIVE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "83f6a687-7fa9-4e42-82f9-a8f72298d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_K = (model.model.transformer.h[i1].mlp.c_fc.weight.cpu() - old_model.model.transformer.h[i1].mlp.c_fc.weight.cpu()).T\n",
    "diff_V = (model.model.transformer.h[i1].mlp.c_proj.weight.cpu() - old_model.model.transformer.h[i1].mlp.c_proj.weight.cpu())\n",
    "diff_WQ, diff_WK, diff_WV = ((model.model.transformer.h[i1].attn.c_attn.weight.cpu() - \n",
    "                              old_model.model.transformer.h[i1].attn.c_attn.weight.cpu()).T.chunk(3))\n",
    "diff_WO = (model.model.transformer.h[i1].attn.c_proj.weight.cpu() - old_model.model.transformer.h[i1].attn.c_proj.weight.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8977bab0-6c83-4ac3-a842-d6d7c93e3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_param = diff_WO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b63e1491-c1d9-46f9-87e2-c5e8de97b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = 11 # this is the layer we visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2c76f6cb-8acd-4753-b0d9-50b59748cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2 = np.random.randint(diff_param.shape[0]) # index of vector in the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e8840f5a-3b4e-4b6a-8cc1-06752ec66c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "diff            -diff\n",
      "--------------  ------------------\n",
      "$               redes\n",
      "#ressed         advoc\n",
      "#t              mathemat\n",
      "expense         horizont\n",
      "#id             #accompan\n",
      "#ah             #isSpecialOrder...\n",
      "P               resil\n",
      "#ank            conduc\n",
      "ID              therap\n",
      "frame           trave\n",
      "#eff            challeng\n",
      "#in             tremend\n",
      "#ham            Parables\n",
      "administration  enthusi\n",
      "security        #ModLoader\n",
      "C               conclud\n",
      "#m              millenn\n",
      "#back           nostalg\n",
      "#um             perspect\n",
      "back            destro\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(zip(*[top_tokens(diff_param[i2].detach() @ emb1, k=20, only_ascii=True, tokenizer=tokenizer),\n",
    "                top_tokens(-diff_param[i2].detach() @ emb1, k=20, only_ascii=True, tokenizer=tokenizer)]), \n",
    "               headers=[\"diff\", \"-diff\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f32cb-12b2-4a07-b778-70fdb9504686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddad447-98da-4efa-b4fe-af1234d0a5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe38cc-26c5-4aea-b966-5b6e98afc3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f1ef7-75d1-4c1d-8964-ce90322fe627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37450a70-3aa2-4228-8132-dbbe2537753c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0c489b-62dd-442c-b7ed-4a2cbdf66fe4",
   "metadata": {},
   "source": [
    "## Model Stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "90c6d7c7-d95f-4acd-b4fe-08e4da1bac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_modules(mod1, mod2, subtract_ln=False):\n",
    "    mod_new = deepcopy(mod1)\n",
    "    with torch.no_grad():\n",
    "        for n, p in mod_new.named_parameters():\n",
    "            submodule_name = n.rsplit('.', 1)[0] if '.' in n else ''\n",
    "            is_ln = isinstance(mod_new.get_submodule(submodule_name), nn.LayerNorm)\n",
    "            if (not is_ln) or subtract_ln:\n",
    "                p.set_(p.data - mod2.get_parameter(n).data)\n",
    "    return mod_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "c78ba6e8-bdd9-4fbf-97a9-d8f4b2b3e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StitchedTransformers(nn.Module):\n",
    "    def __init__(self, old_model, model1, model2, kernel, num_keep_layers, num_transplanted_layers):\n",
    "        super().__init__()\n",
    "        self.model2 = deepcopy(model2) \n",
    "        self.model2.transformer.h = nn.ModuleList(self.model2.transformer.h[:num_keep_layers])\n",
    "        self.register_buffer(\"stitching_kernel\", kernel)     \n",
    "        self.model1 = deepcopy(model1)\n",
    "        offset = len(model1.model.transformer.h) - num_transplanted_layers\n",
    "        self.model1.model.transformer.h = nn.ModuleList([\n",
    "            subtract_modules(model1.model.transformer.h[offset + i], old_model.model.transformer.h[offset + i]) \n",
    "                                                  for i in range(num_transplanted_layers)])\n",
    "        self.model1.model.classifier = subtract_modules(model1.model.classifier, old_model.model.classifier)\n",
    "        \n",
    "    def forward(self, input_ids, labels):\n",
    "        x = self.model2(input_ids, output_hidden_states=True).hidden_states[-1]\n",
    "        x = x @ self.stitching_kernel\n",
    "        res = self.model1(input_ids=None, inputs_embeds=x, labels=labels)\n",
    "        res = {'loss': res['loss'], 'logits': res['logits']}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f57515ac-730f-4a8a-b4c9-2d65dcfb33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = emb2 @ (emb1).pinverse() #+ .1 * torch.eye(1024, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "f5228ca9-3026-4338-973a-f151ea6b5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transplanted_layers = 3\n",
    "num_keep_layers = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5415437-7726-43a3-9823-8af3e9a6ab81",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f0c80b60-6c1a-442b-9e64-0aefda4318ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched_model = StitchedTransformers(old_model.cuda(), model1, model2, kernel, \n",
    "                                      num_keep_layers, num_transplanted_layers).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "271fa53f-ff22-43e6-a471-945629f46082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `StitchedTransformers.forward` and have been ignored: text, attention_mask. If text, attention_mask are not expected by `StitchedTransformers.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6885536909103394,\n",
       " 'eval_accuracy': 0.492,\n",
       " 'eval_runtime': 11.5861,\n",
       " 'eval_samples_per_second': 43.155,\n",
       " 'eval_steps_per_second': 43.155}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stitched = Trainer(stitched_model, args=train_args, train_dataset=imdb_train, eval_dataset=imdb_val, \n",
    "                           compute_metrics=compute_metrics)\n",
    "trainer_stitched.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "4e374f6c-fa48-4e28-8452-cfca66e53f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c429c-cafb-4111-b35a-75a7a35ab36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
